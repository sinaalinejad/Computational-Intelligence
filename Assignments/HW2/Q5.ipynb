{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wYbJSYCpnDnO"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QPYJWK2YL25n"
      },
      "outputs": [],
      "source": [
        "class Sequential:\n",
        "  \"\"\"\n",
        "    This class contains multiple layers in sequence\n",
        "  \"\"\"\n",
        "  def __init__(self, layers, learning_rate):\n",
        "    self.layers = layers # layers of this model\n",
        "    self.lr = learning_rate # learning rate\n",
        "\n",
        "  def forward_pass(self, inputs):\n",
        "    \"\"\"\n",
        "      forward passing to the whole network\n",
        "    \"\"\"\n",
        "    outputs = inputs\n",
        "    for layer in self.layers:\n",
        "      outputs = layer.forward_pass(outputs)\n",
        "    return outputs\n",
        "\n",
        "  def backward_pass(self, derivations):\n",
        "    \"\"\"\n",
        "      backpropagation in the network\n",
        "    \"\"\"\n",
        "    for layer in self.layers[::-1]:\n",
        "      derivations = layer.backward_pass(derivations)\n",
        "\n",
        "  def optimize(self):\n",
        "    \"\"\"\n",
        "      optimizing all the parameters in the network\n",
        "    \"\"\"\n",
        "    for layer in self.layers:\n",
        "      if layer.trainable: # if the layer has trainable parameters\n",
        "        layer.optimize(self.lr) \n",
        "\n",
        "class Dense:\n",
        "  def __init__(self, num_previous_layer_units, num_units):\n",
        "    self.trainable = True # if the layers is trainable\n",
        "    self.units = num_units # number of neurons in this layer\n",
        "    self.prev_units = num_previous_layer_units # number of neurons in the previous layer\n",
        "    # weight initialization\n",
        "    self.weights = np.random.uniform(-1,1,num_previous_layer_units*num_units).reshape((num_previous_layer_units, num_units))\n",
        "    self.biases = np.random.uniform(-1,1,num_units).reshape((1,num_units))\n",
        "  def forward_pass(self, inputs):\n",
        "    \"\"\"\n",
        "      forward passing in this linear layer \n",
        "    \"\"\"\n",
        "    self.last_input = inputs\n",
        "    return np.dot(inputs, self.weights) + self.biases # O = WX + b\n",
        "  def backward_pass(self, derivations):\n",
        "    \"\"\"\n",
        "      backpropagation from this layer\n",
        "    \"\"\"\n",
        "    # computing weight derivations\n",
        "    new_derivations = np.zeros(self.weights.shape) # weight derivations will be stored here\n",
        "    inputs = np.mean(self.last_input, axis=0)\n",
        "    for i in range(new_derivations.shape[0]):\n",
        "      for j in range(new_derivations.shape[1]):\n",
        "          new_derivations[i,j] = inputs[i] * derivations[j]\n",
        "    self.derivations = new_derivations\n",
        "    # computing bias derivations\n",
        "    bias_derivations = np.zeros(self.biases.shape)\n",
        "    for i in range(self.units):\n",
        "      bias_derivations[0,i] = derivations[i]\n",
        "    self.bias_derivations = bias_derivations\n",
        "    # compute the derivations for the next layer in backpropagation\n",
        "    input_derivations = np.zeros(shape=(self.prev_units))\n",
        "    for i in range(len(input_derivations)):\n",
        "      input_derivations[i] = np.sum(derivations * self.weights[i])\n",
        "    return input_derivations\n",
        "  def optimize(self, learning_rate):\n",
        "    \"\"\"\n",
        "      optimizing the parameters of this linear layer based on last derivations calculated in backpropagation function\n",
        "    \"\"\"\n",
        "    self.weights = self.weights - learning_rate * self.derivations\n",
        "    self.biases = self.biases - learning_rate * self.bias_derivations\n",
        "\n",
        "class Sigmoid:\n",
        "  \"\"\"\n",
        "    This is a Sigmoid activation layer\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.trainable = False # this layer does not have any trainable parameter\n",
        "  def forward_pass(self, inputs):\n",
        "    \"\"\"\n",
        "      This function applies forward pass for a layer of sigmoid activation function\n",
        "      inputs: output of previous layer with shape (batch_size, num_prev_layer_units)\n",
        "    \"\"\"\n",
        "    self.last_input = inputs # storing the last input to further use it in backpropagation step\n",
        "    return self.sigmoid(inputs)\n",
        "\n",
        "  def backward_pass(self, derivations):\n",
        "    \"\"\"\n",
        "      Backpropagation for this Sigmoid layer\n",
        "      derivations: this is the derivation of this layer's neurons which was calculated ans passed by previous layer in backward pass\n",
        "    \"\"\"\n",
        "    new_derivations = np.zeros(shape=(self.last_input.shape[1], 1))\n",
        "    inputs = np.mean(self.last_input, axis=0)\n",
        "    inputs_sigmoid = self.sigmoid(inputs)\n",
        "    for i in range(new_derivations.shape[0]):\n",
        "      new_derivations[i,0] = derivations[i] * inputs_sigmoid[i] * (1 - inputs_sigmoid[i]) # formula for derivation of sigmoid is sigmoid * (1 - sigmoid)\n",
        "    return new_derivations\n",
        "\n",
        "  def sigmoid(self, inputs):\n",
        "    \"\"\"\n",
        "      calculating the sigmoid function\n",
        "      formula is 1 / (1 + e^(-x))\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "class Tanh:\n",
        "  def __init__(self):\n",
        "    self.trainable = False # this layer does not have any trainable parameters\n",
        "  def forward_pass(self, inputs):\n",
        "    \"\"\"\n",
        "      This function applies forward pass for a layer of tanh activation function\n",
        "      inputs: output of previous layer with shape (batch_size, num_prev_layer_units)\n",
        "    \"\"\"\n",
        "    self.last_input = inputs\n",
        "    return self.tanh(inputs)\n",
        "\n",
        "  def backward_pass(self, derivations):\n",
        "    \"\"\"\n",
        "      Calculates the derivations for next layer in backpropagation process\n",
        "      derivations: this is the derivation of this layer's neurons which was calculated ans passed by previous layer in backward pass\n",
        "    \"\"\"\n",
        "    new_derivations = np.zeros(shape=(self.last_input.shape[1]))\n",
        "    inputs = np.mean(self.last_input, axis=0)\n",
        "    inputs_tanh = self.tanh(inputs)\n",
        "    for i in range(new_derivations.shape[0]):\n",
        "      new_derivations[i] = derivations[i] * (1 - inputs_tanh[i] * inputs_tanh[i]) # formula for derivation of tanh is 1 - tanh^2\n",
        "    return new_derivations\n",
        "\n",
        "  def tanh(self, inputs):\n",
        "    \"\"\"\n",
        "      calculating the tanh function\n",
        "    \"\"\"\n",
        "    return np.tanh(inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1eUjb9AZzH2V"
      },
      "outputs": [],
      "source": [
        "# defining the model using defined classes\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(2,8),\n",
        "        Tanh(),\n",
        "        Dense(8,4),\n",
        "        Tanh(),\n",
        "        Dense(4,1),\n",
        "        Tanh(),\n",
        "    ],\n",
        "    0.01 # learning rate\n",
        ")\n",
        "# defining the train set\n",
        "x_train = np.array([\n",
        "    [1,1],\n",
        "    [1,-1],\n",
        "    [-1,1],\n",
        "    [-1,-1]\n",
        "])\n",
        "# labels\n",
        "y_train = np.array([1,-1,-1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "07v5raXyEmmz",
        "outputId": "949721aa-9ae5-4523-bd7a-48f6950d8a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0: loss: 1.4457245496940585\n",
            "epoch 50: loss: 0.5148646034534683\n",
            "epoch 100: loss: 0.15829494029293553\n",
            "epoch 150: loss: 0.06402556492133354\n",
            "epoch 200: loss: 0.03546582429838912\n",
            "epoch 250: loss: 0.023324982001497848\n",
            "epoch 300: loss: 0.016943925067009657\n",
            "epoch 350: loss: 0.01311181831082551\n",
            "epoch 400: loss: 0.01059477288353692\n",
            "epoch 450: loss: 0.008832612224242328\n",
            "epoch 500: loss: 0.007538917697088247\n",
            "epoch 550: loss: 0.006553690709543981\n",
            "epoch 600: loss: 0.005781237591529308\n",
            "epoch 650: loss: 0.005161137598347931\n",
            "epoch 700: loss: 0.004653524339122707\n",
            "epoch 750: loss: 0.0042311171215664205\n",
            "epoch 800: loss: 0.00387466654428798\n",
            "epoch 850: loss: 0.0035702304739605303\n",
            "epoch 900: loss: 0.0033074805862786023\n",
            "epoch 950: loss: 0.003078613961362905\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1000\n",
        "losses = []\n",
        "for i in range(num_epochs):\n",
        "  avg_loss = 0\n",
        "  for j in range(4):\n",
        "    output = model.forward_pass([x_train[j],])\n",
        "    loss = (y_train[j] - output[0][0]) ** 2\n",
        "    derivation = (output[0][0] - y_train[j])\n",
        "    avg_loss += loss\n",
        "    model.backward_pass(np.array([derivation,]))\n",
        "    model.optimize()\n",
        "  avg_loss /= 4\n",
        "  losses.append(avg_loss)\n",
        "  if i % 50 == 0:\n",
        "    print(f\"epoch {i}: loss: {avg_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x20675be8490>]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(num_epochs), losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq0jvifMnTyf",
        "outputId": "83ffa40d-a9df-4f1a-810e-66a5950de1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 1.0\n"
          ]
        }
      ],
      "source": [
        "# calculating the accuracy\n",
        "outputs = model.forward_pass(x_train)\n",
        "accuracy = 0\n",
        "for i in range(4):\n",
        "  if outputs[i] > 0:\n",
        "    final = 1\n",
        "  else:\n",
        "    final = -1\n",
        "  if y_train[i] == final:\n",
        "    accuracy += 1\n",
        "print(f\"accuracy = {accuracy / 4}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
